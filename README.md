# RandomForestFromScratch-EMNIST
Random Forest classifier implementation from scratch with Python3 as a part of EEE 485 Statistical Learning and Data Analytics term project. It is trained on EMNIST dataset with both the original dataset as well as feature extracted version, with Histogram of Oriented Gradients. A comparison of Random Forest, Support Vector Machine and Feedforward Neural Network is listed in Table 1. Random Forest is rather slow on large dataset, but it is more intuitive to play with its hyperparameters compared to a neural network.

<img width="996" alt="Screen Shot 2021-05-17 at 17 34 19" src="https://user-images.githubusercontent.com/77360680/118507650-1b0f3d80-b737-11eb-9225-f5d7cb34bfc7.png">


### Simulation Setup and Final Results 

To setup the random forest algorithm, we have first generated a single decision tree whose performance can easily be tracked by printing the tree nodes. To make sure that the decision tree works well, a small handcrafted dataset with few features and few samples is used. After making sure that decision tree splits the given dataset with reasonable splits, we moved on the random forest. To simulate the random forest, we defined some hyperparameters such as number of decision trees in the forest, dataset size of each tree will train on and number of features each tree can pick from the dataset. For various hyperparameters, we have trained different random forests and tabulated their results as in Table 2. To evaluate how the forest works, before moving on the EMNIST test dataset, the fixed 80% training set for bootstrap
dataset of the random forests is used. For the hyperparameter tuning, the fixed validation set is used for all the
models.

At the preliminary simulations, we were using only around 12000 samples due to the training time considerations. After improving the speed of the algorithm and reducing the number of features, we were able to use all 90240 samples available in 80% of the EMNIST balanced training dataset. Table 2 includes the best validation accuracies trained on raw pixels dataset. In addition, we have optimized of threshold trials while finding the best splits, to reduce the computation time. This optimization linearly reduced the time for best split search; however, the accuracy is slightly reduced compared to greedy search results as well. The decision trees are prone to overfitting. To prevent that, we have put a depth limit for each tree in order not to classify based on the noise of the data. In addition, during the simulation setup at the first demo, we saw that some features are the same for all samples, e.g. the black pixels at the corners. Therefore, in order not to try to find the best split with respect to such redundant feature, we are checking if the randomly selected feature is the same for all samples before using it.

For the speed optimization, we have also replaced for-loops in some computations with numpy implementation which reduced down the speed of training as well. However, the major improvement in both training speed and the accuracy was due to the feature exraction and reduction from HOG method. As a sidenote, the best HOG model’s training time at the Table 2 could be even less. The training time increases linearly with respect to the model parameters, which can be observed in the Table 2 as well. However, some of the models were trained on Google Colab, while the others were running on a Jupyter Notebook. Hence, the ones trained with Colab have even less training time with the local computer we were using. According to Table 2, 1000 trees trained on 2048 randomly selected training samples with replacement focusing on 20 random features model gives the most accurate result, around 74% validation accuracy. Therefore, we have selected this model to be tested on the EMNIST balanced test set in the end. 

In summary, the test results can be inspected from the confusion matrix given at figure Appendix 20. Confusion matrix is a useful tool for classification to see where the model confuses the true labels. Since the rough decision trees are generalizing the model well, its disadvantage occurs between similar characters such as ”O”,”0” or ”L”,”I”, and ”1”. The HOG dataset we are using mostly eliminates the nuances between such characters since its pixels per block is relatively high; however, this dataset provided critical information that the trees could use to classify the characters better.

<img width="746" alt="Screen Shot 2021-05-17 at 17 42 47" src="https://user-images.githubusercontent.com/77360680/118508594-01bac100-b738-11eb-8850-78608ceaf5da.png">

<img width="456" alt="Screen Shot 2021-05-17 at 17 47 22" src="https://user-images.githubusercontent.com/77360680/118508584-fff0fd80-b737-11eb-9ed8-d2ff7208ddc5.png">
